1 hadoop 单机配置
/usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"

分析单词出现的次数
./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount oo xx

#-----------------------------------------------------#
ALL 表示所有主机，
NODE 表示 node1,node2,node3
NN1: 表示 namenode
#-----------------------------------------------------#

完全分布式集群搭建 -- HDFS
192.168.1.10	nn01	namenode,secondarynamenode
192.168.1.11	node1	datanode
192.168.1.12	node2	datanode
192.168.1.13	node3	datanode

ALL: 配置 /etc/hosts
ALL: 安装 java-1.8.0-openjdk-devel

core-site.xml 配置
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nn01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/hadoop</value>
    </property>
</configuration>

ALL: 创建 /var/hadoop

hdfs-site.xml 配置
<configuration>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>nn01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>nn01:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
</configuration>

ALL: 同步配置到所有主机

NN01: 格式化 namenode
./bin/hdfs namenode -format

NN01: 启动集群
./sbin/start-dfs.sh
停止集群可以使用 ./sbin/stop-dfs.sh

ALL: 验证角色 jps

NN01: 验证集群是否组建成功
./bin/hdfs dfsadmin -report

服务启动日志路径 /usr/local/hadoop/logs

mapred-site.xml 配置
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

yarn-site.xml 配置
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>nn01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

ALL: 同步配置到主机
NN1: 启动服务 ./sbin/start-yarn.sh
ALL: 验证角色 jps 
NN1: 验证节点状态 ./bin/yarn node -list

增加修复节点
按照单机方法安装一台机器，部署运行的 java 环境
拷贝 namenode 的文件到本机
启动 datanode
./sbin/hadoop-daemons.sh start datanode
设置同步带宽
./bin/hdfs dfsadmin -setBalancerBandwidth 60000000
./sbin/start-balancer.sh

删除节点
<property>
    <name>dfs.hosts.exclude</name>
    <value>/usr/local/hadoop/etc/hadoop/exclude</value>
</property>

开始导出数据
./bin/hdfs dfsadmin -refreshNodes

查看状态
Normal   正常状态
Decommissioned  in  Program  数据正在迁移
Decommissioned   数据迁移完成

yarn 增加 nodemanager
./sbin/yarn-daemon.sh start nodemanager
yarn 停止 nodemanager
./sbin/yarn-daemon.sh stop  nodemanager
yarn 查看节点状态
./bin/yarn node -list

NFS 网关
1 配置 /etc/hosts (NFSGW)
192.168.1.10	nn01
192.168.1.11	node1
192.168.1.12	node2
192.168.1.13	node3
192.168.1.15	nfsgw

2 添加用户(nfsgw, nn01)
groupadd -g 200 nsd1803
useradd -u 200 -g 200 nsd1803

NN01: 3 停止集群
./sbin/stop-all.sh

NN01: 4 增加配置 core-site.xml
    <property>
        <name>hadoop.proxyuser.nsd1803.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.hosts</name>
        <value>*</value>
    </property>
NN01: 5 同步配置到 node1 node2 node3
NN01: 6 启动集群   ./sbin/start-dfs.sh
NN01: 7 查看状态
./bin/hdfs dfsadmin -report

NFSGW: 安装 java-1.8.0-openjdk-devel
NFSGW: 同步 nn01 的 /usr/local/hadoop 到NFSGW的相同目录下
NFSGW: hdfs-site.xml 增加配置
    <property>
        <name>nfs.exports.allowed.hosts</name>
        <value>* rw</value>
    </property>
    <property>
        <name>nfs.dump.dir</name>
        <value>/var/nfstmp</value>
    </property>

NFSGW: 创建转储目录，并给用户 nsd1803 赋权
mkdir /var/nfstmp
chown nsd1803:nsd1803 /var/nfstmp

NFSGW: 给 /usr/local/hadoop/logs 赋权
setfacl -m u:nsd1803:rwx

创建数据根目录 /var/hadoop
mkdir /var/hadoop

Client: 安装 nfs-utils 
mount 共享目录
mount -t nfs -o vers=3,proto=tcp,nolock,noatime,sync,noacl 192.168.1.15:/  /mnt/

Client: 实现开机自动挂载

查看注册服务
rpcinfo -p 192.168.1.15
查看共享目录
showmount -e 192.168.1.15

zookeeper 安装
1 配置 /etc/hosts ,所有集群主机可以相互 ping 通
2 安装 java-1.8.0-openjdk-devel
3 zookeeper 解压拷贝到 /usr/local/zookeeper
4 配置文件改名，并在最后添加配置
mv zoo_sample.cfg  zoo.cfg
zoo.cfg 最后添加配置
server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888
server.4=nn01:2888:3888:observer
5 拷贝 /usr/local/zookeeper 到其他集群主机
6 创建 mkdir /tmp/zookeeper
ALL: 7 创建 myid 文件，id 必须与配置文件里主机名对应的 server.(id) 一致 echo 1 >
/tmp/zookeeper/myid
8 启动服务，单启动一台无法查看状态，需要启动全部集群以后才能查看状态
/usr/local/zookeeper/bin/zkServer.sh start
9 查看状态
/usr/local/zookeeper/bin/zkServer.sh status

利用 api 查看状态的脚本
#!/bin/bash
function getstatus(){
    exec 9<>/dev/tcp/$1/2181 2>/dev/null
    echo stat >&9
    MODE=$(cat <&9 |grep -Po "(?<=Mode:).*")
    exec 9<&-
    echo ${MODE:-NULL}
}
for i in node{1..3} nn01;do
    echo -ne "${i}\t"
    getstatus ${i}
done

kafka 搭建
1 下载解压 kafka 压缩包
2 把 kafka 拷贝到 /usr/local/kafka 下面
3 修改配置文件 /usr/local/kafka/config/server.properties
broker.id=11
zookeeper.connect=node1:2181,node2:2181,node3:2181
4 拷贝 kafka 到其他主机，并修改 broker.id ,不能重复
5 启动 kafka 集群
/usr/local/kafka/bin/kafka-server-start.sh -daemon
/usr/local/kafka/config/server.properties

验证集群
创建一个 topic 
./bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --zookeeper node3:2181 --topic nsd1803
生产者
./bin/kafka-console-producer.sh --broker-list node2:9092 --topic nsd1803
消费者
./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic nsd1803

hadoop 高可用
ALL: 配置 /etc/hosts
192.168.1.10	nn01
192.168.1.20	nn02
192.168.1.11	node1
192.168.1.12	node2
192.168.1.13	node3
ALL: 除了 zookeeper 其他 hadoop ，kafka 服务全部停止
ALL: 初始化 hdfs 集群，删除 /var/hadoop/*
NN2: 关闭 ssh key 验证，部署公钥私钥
StrictHostKeyChecking no
scp nn01:/root/.ssh/id_rsa /root/.ssh/
scp nn01:/root/.ssh/authorized_keys /root/.ssh/

NN1: 配置 core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name> #指定Namenode在哪，或指定namenode组名
        <value>hdfs://nsdcluster</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name> #hadoop 数据存放节点
        <value>/var/hadoop</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>  #zookeeper集群地址
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.groups</name> #
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.hosts</name>
        <value>*</value>
    </property>
</configuration>

配置 hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name> #数据分片数量，2为分3份
        <value>2</value>
    </property>
    <property>
        <name>dfs.nameservices</name> #指定dhfs服务名称为nsdcluster
        <value>nsdcluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.nsdcluster</name> 指定nsdcluster集群的两个namenode
        <value>nn1,nn2</value>
    </property>
    <property>
	#配置nn01，nn02的rpc通信端口
        <name>dfs.namenode.rpc-address.nsdcluster.nn1</name>
        <value>nn01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.nsdcluster.nn2</name>
        <value>nn02:8020</value>
    </property>
    <property>
	#配置nn01，nn02的http通信端口
        <name>dfs.namenode.http-address.nsdcluster.nn1</name>
        <value>nn01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.nsdcluster.nn2</name>
        <value>nn02:50070</value>
    </property>
    <property>
	#指定namenode元数据存储在journalnode中的路径
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://node1:8485;node2:8485;node3:8485/nsdcluster</value>
    </property>
    <property>
	#指定journnode日志文件存储路径
        <name>dfs.journalnode.edits.dir</name>
        <value>/var/hadoop/journal</value>
    </property>
    <property>
	#指定hdfs客户端连接active namenode的java类
        <name>dfs.client.failover.proxy.provider.nsdcluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
	#隔离机制为ssh，管理集群的方式
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
	#指定密钥的位置
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>
    <property>
	#开启自动故障转移
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
</configuration>

yarn-site.xml 配置文件
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
	#打开yarn.配置
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property> 
    <property>
        #申明两个resourcemanager节点,对应nn01,nn02
	<name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
	#打开服务
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
	#resourcemanager在zookeeper上存储信息的类
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
	#指定zookeeper服务地址
        <name>yarn.resourcemanager.zk-address</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
	#
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarn-ha</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>nn01</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>nn02</value>
    </property>
</configuration>

#-----------------------------------------------------#
初始化启动集群
ALL: 所有机器
nodeX： node1    node2    node3
NN1: nn01
NN2: nn02
#-----------------------------------------------------#
ALL:  同步配置到所有集群机器

NN1: 初始化ZK集群  ./bin/hdfs zkfc -formatZK

nodeX:  启动 journalnode 服务 
        ./sbin/hadoop-daemon.sh start journalnode

NN1: 格式化  ./bin/hdfs  namenode  -format

NN2: 数据同步到本地 /var/hadoop/dfs

NN1: 初始化 JNS
        ./bin/hdfs namenode -initializeSharedEdits

nodeX: 停止 journalnode 服务
        ./sbin/hadoop-daemon.sh stop journalnode

#-----------------------------------------------------#
启动集群
NN1: ./sbin/start-all.sh
NN2: ./sbin/yarn-daemon.sh start resourcemanager

查看集群状态
./bin/hdfs haadmin -getServiceState nn1  
./bin/hdfs haadmin -getServiceState nn2
./bin/yarn rmadmin -getServiceState rm1
./bin/yarn rmadmin -getServiceState rm2

./bin/hdfs dfsadmin -report
./bin/yarn  node  -list

访问集群：
./bin/hadoop  fs -ls  /
./bin/hadoop  fs -mkdir hdfs://nsdcluster/input

验证高可用，关闭 active namenode
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

恢复节点
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

